{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init network functoin\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "    network = list()\n",
    "    hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed\n",
    "from random import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}]\n",
      "[{'weights': [0.2550690257394217, 0.49543508709194095]}, {'weights': [0.4494910647887381, 0.651592972722763]}]\n"
     ]
    }
   ],
   "source": [
    "seed(1)\n",
    "network = initialize_network(2,1,2)\n",
    "for layer in network:\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 hidden layer, with two weights plus bias\n",
    "ouput layer with two neurons each with 1 wiehgt plus a bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7887233511355132, 0.0938595867742349, 0.02834747652200631]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[random() for i in range(3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propogate\n",
    "\n",
    "carry input through the network\n",
    "\n",
    "need neuron activation\n",
    "nueron transfer\n",
    "and forward prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate neruon activation for an input\n",
    "def activate(weights, inputs):\n",
    "    activation = weights[-1]\n",
    "    for i in range(len(weights)-1):\n",
    "        activation += weights[i]*inputs[i]\n",
    "    return(activation)\n",
    "\n",
    "#transfer function\n",
    "def transfer(activation):\n",
    "    return(1.0 / (1.0 + exp(-activation)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we work through each layer of our network calculatin the outputs for each neruone. all the outputs from one layer become inputs to the neurons on the next layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propogate(network, row):\n",
    "    inputs = row\n",
    "    for layer in network:\n",
    "        new_inputs = []\n",
    "        for neuron in layer:\n",
    "            activation = activate(neuron['weights'],inputs)\n",
    "            neuron['output'] = transfer(activation)\n",
    "            new_inputs.append(neuron['output'])\n",
    "        inputs = new_inputs\n",
    "    return(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.648629545757926, 0.7969578641752217]\n"
     ]
    }
   ],
   "source": [
    "from math import exp\n",
    "# test forward propagation\n",
    "network = initialize_network(2,1,2)\n",
    "row = [1, 0, None]\n",
    "output = forward_propogate(network, row)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wooohoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back propagate error\n",
    "\n",
    "way to train the weights\n",
    "\n",
    "eror is calculated between the expected outputs and the outpts forward propagate from the network. These errors are then propogate backward through the netowrk form the output layer to the hidden layer, assignling blacem for the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transfer derivative\n",
    "def transfer_derivative(output):\n",
    "    return(output * (1.0 - output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error propogation:\n",
    "\n",
    "error = (exepcted - output)*transferddeivatie(output)\n",
    "\n",
    "which is just chain rule wrt to the ouput\n",
    "\n",
    "the error signal for a neuron in the hidden layer is calculated as the weighted error of each neurone in the output layer. think of the error traveling back along the weights of the ouput layer to the neurons in the hidden layer\n",
    "\n",
    "error = (weight_k * error_j) * transfer_derivative(output)\n",
    "\n",
    "where error_j is the error signal from the jth neurone in the output layer, weight_k is the weight that connects the kth neuorn to the current neuron and output if the outpur for the current neueron\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagate error and store in neurons\n",
    "def backward_propagate_error(network, expected):\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        if i != len(network)-1:\n",
    "            for j in range(len(layer)):\n",
    "                error = 0.0\n",
    "                for neuron in network[i + 1]:\n",
    "                    error += (neuron['weights'][j] * neuron['delta'])\n",
    "                errors.append(error)\n",
    "        else:\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                errors.append(expected[j] - neuron['output'])\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'output': 0.7105668883115941, 'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614], 'delta': -0.0005348048046610517}]\n",
      "[{'output': 0.6213859615555266, 'weights': [0.2550690257394217, 0.49543508709194095], 'delta': -0.14619064683582808}, {'output': 0.6573693455986976, 'weights': [0.4494910647887381, 0.651592972722763], 'delta': 0.0771723774346327}]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the derivative of an neuron output\n",
    "def transfer_derivative(output):\n",
    "    return output * (1.0 - output)\n",
    " \n",
    "# Backpropagate error and store in neurons\n",
    "def backward_propagate_error(network, expected):\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        if i != len(network)-1:\n",
    "            for j in range(len(layer)):\n",
    "                error = 0.0\n",
    "                for neuron in network[i + 1]:\n",
    "                    error += (neuron['weights'][j] * neuron['delta'])\n",
    "                errors.append(error)\n",
    "        else:\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                errors.append(expected[j] - neuron['output'])\n",
    "        for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
    " \n",
    "#test backpropagation of error\n",
    "network = [[{'output': 0.7105668883115941, 'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}],\n",
    "\t\t[{'output': 0.6213859615555266, 'weights': [0.2550690257394217, 0.49543508709194095]}, {'output': 0.6573693455986976, 'weights': [0.4494910647887381, 0.651592972722763]}]]\n",
    "expected = [0, 1]\n",
    "backward_propagate_error(network, expected)\n",
    "for layer in network:\n",
    "\tprint(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2550690257394217, 0.49543508709194095]\n",
      "-0.14619064683582808\n",
      "[0.4494910647887381, 0.651592972722763]\n",
      "0.0771723774346327\n"
     ]
    }
   ],
   "source": [
    "for i in network[0+1]:\n",
    "    print(i['weights'])\n",
    "    print(i['delta'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network\n",
    "\n",
    "the network is trained using stoachsit gradient decent\n",
    "\n",
    "this involves using mulitple iterations of exposing a training dataset to the network and for each row of data forward propgating the inputs, backpropgating the error graidents and updating the weights\n",
    "\n",
    "defined as:\n",
    "update wieghts\n",
    "train network\n",
    "\n",
    "weight = weight + learning_rate * error * input\n",
    "\n",
    "notes on learning rates, it controls how much to change the weight vector. for example, a small value of 0.1 will update 10% of the amount of time it possibly could be updated.\n",
    "\n",
    "small learning rates are preferred that cause slower learning over a larger number of training iterations. This increase the likelihood of the network finding a good set of weight across all layers rather than the fastest set of weights that minimuze the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(network, row, l_rate):\n",
    "    for i in range(len(network)):\n",
    "        inputs = row[:-1]\n",
    "        if i != 0:\n",
    "            inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                neuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
    "            neuron['weights'][-1] += l_rate * neuron['delta']\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
